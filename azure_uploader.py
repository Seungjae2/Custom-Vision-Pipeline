# azure_uploader.py

import os
import json
import base64
import requests
import time
from urllib.parse import quote
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# --- ì„¤ì • (ì—…ë¡œë“œ/í•™ìŠµìš© í”„ë¡œì íŠ¸ Bì˜ ì •ë³´ ì‚¬ìš©) ---
TRAINING_KEY = os.getenv("AZURE_TRAINING_KEY")
TRAINING_ENDPOINT = os.getenv("AZURE_TRAINING_ENDPOINT")
PROJECT_ID = os.getenv("AZURE_UPLOADER_PROJECT_ID")
PREDICTION_RESOURCE_ID = os.getenv("AZURE_PREDICTION_RESOURCE_ID")
USE_ADVANCED_TRAINING = True

TRAIN_HEADERS = {"Training-Key": TRAINING_KEY, "Content-Type": "application/json"}


# --- ì‹ ê·œ í•¨ìˆ˜: Azure í”„ë¡œì íŠ¸ì˜ ê¸°ì¡´ ì´ë¯¸ì§€ ëª©ë¡ ì¡°íšŒ ---
def get_existing_images_from_azure():
    """Azure Custom Vision í”„ë¡œì íŠ¸ì— ì´ë¯¸ ì—…ë¡œë“œëœ ëª¨ë“  ì´ë¯¸ì§€ì˜ íŒŒì¼ ì´ë¦„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    existing_images = set()
    page_num = 0

    print("\nâ˜ï¸ Azureì—ì„œ ê¸°ì¡´ ì´ë¯¸ì§€ ëª©ë¡ì„ í™•ì¸í•©ë‹ˆë‹¤...")

    while True:
        # get_tagged_images APIëŠ” í˜ì´ì§• ì²˜ë¦¬ê°€ í•„ìš” (í•œ ë²ˆì— ìµœëŒ€ 256ê°œì”© ì¡°íšŒ)
        url = f"{TRAINING_ENDPOINT}customvision/v3.3/training/projects/{PROJECT_ID}/images?take=256&skip={page_num * 256}"
        try:
            res = requests.get(url, headers=TRAIN_HEADERS)
            res.raise_for_status()
            images_on_page = res.json()

            if not images_on_page:
                break  # ë” ì´ìƒ ê°€ì ¸ì˜¬ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ë£¨í”„ ì¢…ë£Œ

            for image in images_on_page:
                # AzureëŠ” íŒŒì¼ ì´ë¦„ì„ 'name' í•„ë“œì— ì €ì¥
                if 'name' in image:
                    existing_images.add(image['name'])
            page_num += 1

        except requests.exceptions.RequestException as e:
            print(f"âŒ ê¸°ì¡´ ì´ë¯¸ì§€ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None  # ì˜¤ë¥˜ ë°œìƒ ì‹œ None ë°˜í™˜

    print(f"  - âœ… {len(existing_images)}ê°œì˜ ê¸°ì¡´ ì´ë¯¸ì§€ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")
    return existing_images


def sync_and_get_tags(required_tag_names):
    """
    Azure í”„ë¡œì íŠ¸ì˜ íƒœê·¸ë¥¼ ë™ê¸°í™”í•˜ê³  ìµœì¢… íƒœê·¸ ë§µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print("\nğŸ”„ Azure í”„ë¡œì íŠ¸ì™€ íƒœê·¸ ë™ê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    get_tags_url = f"{TRAINING_ENDPOINT}customvision/v3.3/training/projects/{PROJECT_ID}/tags"
    try:
        res = requests.get(get_tags_url, headers=TRAIN_HEADERS)
        res.raise_for_status()
        tag_map = {tag['name']: tag['id'] for tag in res.json()}
        print(f"  - í˜„ì¬ í”„ë¡œì íŠ¸ì— {len(tag_map)}ê°œì˜ íƒœê·¸ê°€ ìˆìŠµë‹ˆë‹¤: {list(tag_map.keys())}")
    except requests.exceptions.RequestException as e:
        print(f"âŒ ê¸°ì¡´ íƒœê·¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None

    for tag_name in required_tag_names:
        if tag_name not in tag_map:
            print(f"  - í•„ìš”í•œ íƒœê·¸ '{tag_name}'ì„(ë¥¼) ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
            create_tag_url = f"{TRAINING_ENDPOINT}customvision/v3.3/training/projects/{PROJECT_ID}/tags?name={quote(tag_name)}"
            try:
                res = requests.post(create_tag_url, headers={"Training-Key": TRAINING_KEY})
                res.raise_for_status()
                new_tag_info = res.json()
                tag_map[new_tag_info['name']] = new_tag_info['id']
                print(f"  âœ… ìƒˆë¡œìš´ íƒœê·¸ ìƒì„± ì„±ê³µ: {new_tag_info['name']}")
            except requests.exceptions.RequestException as e:
                print(f"  âŒ íƒœê·¸ '{tag_name}' ìƒì„± ì‹¤íŒ¨: {e}")
                continue

    print("  - âœ… íƒœê·¸ ë™ê¸°í™” ì™„ë£Œ.")
    return tag_map


def get_next_iteration_name():
    # ... (ì´ì „ê³¼ ë™ì¼, ë³€ê²½ ì—†ìŒ)
    url = f"{TRAINING_ENDPOINT}customvision/v3.3/training/projects/{PROJECT_ID}/iterations"
    try:
        res = requests.get(url, headers=TRAIN_HEADERS);
        res.raise_for_status();
        iterations = res.json()
    except requests.exceptions.RequestException:
        return "Iteration-1"
    max_num = 0
    for it in iterations:
        if "Iteration" in it["name"]:
            try:
                num_part = it["name"].replace("Iteration", "").strip().replace("-", "");
                num = int(num_part);
                max_num = max(max_num, num)
            except (ValueError, IndexError):
                continue
    return f"Iteration-{max_num + 1}"


def convert_coco_to_azure_format(coco_data, tag_map):
    # ... (ì´ì „ê³¼ ë™ì¼, ë³€ê²½ ì—†ìŒ)
    uploads = {};
    images = {img["id"]: img for img in coco_data["images"]};
    categories = {cat["id"]: cat["name"] for cat in coco_data["categories"]}
    for ann in coco_data["annotations"]:
        image_info = images.get(ann["image_id"]);
        file_name = image_info["file_name"] if image_info else None
        category_name = categories.get(ann["category_id"])
        if file_name and category_name and category_name in tag_map:
            x, y, w, h = ann["bbox"];
            width, height = image_info["width"], image_info["height"]
            region = {"tagId": tag_map[category_name], "left": x / width, "top": y / height, "width": w / width,
                      "height": h / height}
            if file_name not in uploads: uploads[file_name] = []
            uploads[file_name].append(region)
    return uploads


def upload_images_to_azure(image_folder, uploads):
    # ... (ì´ì „ê³¼ ë™ì¼, ë³€ê²½ ì—†ìŒ)
    batch, total_sent = [], 0;
    total_images = len(uploads)
    for fname, regions in list(uploads.items()):
        fpath = os.path.join(image_folder, fname)
        if not os.path.exists(fpath): continue
        with open(fpath, "rb") as f:
            b64_content = base64.b64encode(f.read()).decode()
        batch.append({"name": fname, "contents": b64_content, "regions": regions})
        if len(batch) >= 64: total_sent = send_batch(batch, total_sent, total_images); batch = []
    if batch: send_batch(batch, total_sent, total_images)


def send_batch(batch, sent_count, total_count):
    # ... (ì´ì „ê³¼ ë™ì¼, ë³€ê²½ ì—†ìŒ)
    url = f"{TRAINING_ENDPOINT}customvision/v3.3/training/projects/{PROJECT_ID}/images/files"
    print(f"ğŸ“¤ ì—…ë¡œë“œ ì¤‘... [{sent_count + 1}â€“{sent_count + len(batch)} / {total_count}]")
    try:
        res = requests.post(url, headers=TRAIN_HEADERS, json={"images": batch}, timeout=180);
        res.raise_for_status();
        print("  - ë°°ì¹˜ ì—…ë¡œë“œ ì„±ê³µ.")
    except requests.exceptions.RequestException as e:
        print(f"  - âŒ ë°°ì¹˜ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
    return sent_count + len(batch)


def train_new_iteration(iteration_name):
    # ... (ì´ì „ê³¼ ë™ì¼, ë³€ê²½ ì—†ìŒ)
    url = f"{TRAINING_ENDPOINT}customvision/v3.3/training/projects/{PROJECT_ID}/train?advancedTraining={'true' if USE_ADVANCED_TRAINING else 'false'}"
    res = requests.post(url, headers=TRAIN_HEADERS)
    if res.ok:
        print(f"ğŸ§  í•™ìŠµ ìš”ì²­ ì„±ê³µ: '{iteration_name}'"); return res.json()
    else:
        print(f"âŒ í•™ìŠµ ìš”ì²­ ì‹¤íŒ¨ ({res.status_code}): {res.text}"); return None


def wait_for_training_completion(iteration_id, timeout=3600, interval=30):
    # ... (ì´ì „ê³¼ ë™ì¼, ë³€ê²½ ì—†ìŒ)
    url = f"{TRAINING_ENDPOINT}customvision/v3.3/training/projects/{PROJECT_ID}/iterations/{iteration_id}";
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            res = requests.get(url, headers=TRAIN_HEADERS);
            res.raise_for_status();
            iteration_data = res.json();
            status = iteration_data["status"]
            print(f"â³ í•™ìŠµ ìƒíƒœ í™•ì¸: {status} (ê²½ê³¼ ì‹œê°„: {int(time.time() - start_time)}ì´ˆ)")
            if status == "Completed": return True
            if status in ["Failed", "Canceled"]: return False
            time.sleep(interval)
        except requests.exceptions.RequestException as e:
            print(f"  - í•™ìŠµ ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"); time.sleep(interval)
    print("â° í•™ìŠµ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼");
    return False


def publish_iteration(iteration_id, iteration_name):
    # ... (ì´ì „ê³¼ ë™ì¼, ë³€ê²½ ì—†ìŒ)
    encoded_iteration_name = quote(iteration_name)
    url = f"{TRAINING_ENDPOINT}customvision/v3.3/training/projects/{PROJECT_ID}/iterations/{iteration_id}/publish?publishName={encoded_iteration_name}&predictionId={PREDICTION_RESOURCE_ID}"
    res = requests.post(url, headers=TRAIN_HEADERS)
    if res.ok:
        print(f"ğŸš€ ê²Œì‹œ ì„±ê³µ: '{iteration_name}'")
    else:
        print(f"âŒ ê²Œì‹œ ì‹¤íŒ¨ ({res.status_code}): {res.text}")


# --- ì´ ì•„ë˜ run_uploader í•¨ìˆ˜ê°€ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤ ---
def run_uploader(image_folder, coco_file_path):
    """COCO íŒŒì¼ê³¼ ì´ë¯¸ì§€ í´ë”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Azure ì—…ë¡œë“œ ë° í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    if not all([TRAINING_KEY, TRAINING_ENDPOINT, PROJECT_ID, PREDICTION_RESOURCE_ID]):
        print("âŒ .env íŒŒì¼ì— Azure ì„¤ì •ê°’ì´ ëª¨ë‘ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.");
        return False

    print(f"\nâ–¶ï¸ Azure Uploader ì‹œì‘ (ëŒ€ìƒ íŒŒì¼: {coco_file_path})")
    if not os.path.exists(coco_file_path):
        print(f"âŒ COCO íŒŒì¼({coco_file_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.");
        return False

    with open(coco_file_path, 'r', encoding='utf-8') as f:
        coco_data = json.load(f)

    # 1. Azureì—ì„œ ê¸°ì¡´ ì´ë¯¸ì§€ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    existing_images_on_azure = get_existing_images_from_azure()
    if existing_images_on_azure is None:
        print("âŒ Azureì—ì„œ ì´ë¯¸ì§€ ëª©ë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í•´ ì—…ë¡œë“œë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.");
        return False

    # 2. ë¡œì»¬ JSON íŒŒì¼ì— ìˆëŠ” ì´ë¯¸ì§€ ëª©ë¡ê³¼ ë¹„êµí•˜ì—¬, ì—…ë¡œë“œí•  ìƒˆë¡œìš´ ì´ë¯¸ì§€ ëª©ë¡ì„ ë§Œë“­ë‹ˆë‹¤.
    all_local_images = {img['file_name'] for img in coco_data.get('images', [])}
    new_images_to_upload = all_local_images - existing_images_on_azure

    if not new_images_to_upload:
        print("\nâœ… ìƒˆë¡œìš´ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ì—…ë¡œë“œ ë° í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return True

    print(f"\nğŸ†• ì´ {len(all_local_images)}ê°œ ì´ë¯¸ì§€ ì¤‘ {len(new_images_to_upload)}ê°œì˜ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•©ë‹ˆë‹¤.")

    # 3. ìƒˆë¡œìš´ ì´ë¯¸ì§€ë§Œ í¬í•¨í•˜ë„ë¡ coco_data í•„í„°ë§
    filtered_coco = {"images": [], "annotations": [], "categories": coco_data["categories"]}
    new_image_ids = set()
    for img in coco_data["images"]:
        if img["file_name"] in new_images_to_upload:
            filtered_coco["images"].append(img)
            new_image_ids.add(img["id"])

    for ann in coco_data.get("annotations", []):
        if ann["image_id"] in new_image_ids:
            filtered_coco["annotations"].append(ann)

    if not filtered_coco.get("annotations"):
        print("âš ï¸ ìƒˆë¡œìš´ ì´ë¯¸ì§€ì— ëŒ€í•œ ì£¼ì„(annotation) ë°ì´í„°ê°€ ì—†ì–´ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.");
        return True

    required_tag_names = [cat['name'] for cat in filtered_coco.get('categories', [])]
    tag_map = sync_and_get_tags(required_tag_names)
    if tag_map is None:
        print("âŒ íƒœê·¸ ë§µì„ ê°€ì ¸ì˜¤ì§€ ëª»í•´ ì—…ë¡œë“œë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.");
        return False

    uploads = convert_coco_to_azure_format(filtered_coco, tag_map)
    if not uploads:
        print("âš ï¸ ì—…ë¡œë“œí•  ìœ íš¨í•œ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.");
        return True

    upload_images_to_azure(image_folder, uploads)

    iteration_name = get_next_iteration_name()
    iteration_info = train_new_iteration(iteration_name)

    if iteration_info:
        iteration_id = iteration_info["id"]
        if wait_for_training_completion(iteration_id):
            publish_iteration(iteration_id, iteration_name)
        else:
            print("âš ï¸ í•™ìŠµì´ ì™„ë£Œë˜ì§€ ì•Šì•„ ê²Œì‹œë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
    return True


if __name__ == "__main__":
    # ... (ì´ì „ê³¼ ë™ì¼, ë³€ê²½ ì—†ìŒ)
    print("--- ì—…ë¡œë” ëª¨ë“ˆ ë‹¨ë… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ---")
    print("í¬ë¡¤ë§ìœ¼ë¡œ ìƒì„±ëœ ìµœì‹  ë°ì´í„° í´ë”ì™€ JSON íŒŒì¼ì„ ìë™ìœ¼ë¡œ íƒìƒ‰í•©ë‹ˆë‹¤...")
    base_data_dir = 'data';
    latest_crawled_folder = None
    if os.path.exists(base_data_dir):
        all_subdirs = [d for d in os.listdir(base_data_dir) if
                       os.path.isdir(os.path.join(base_data_dir, d)) and d.startswith('youtube_trending_')]
        if all_subdirs: latest_crawled_folder = max(all_subdirs)

    if latest_crawled_folder:
        image_folder_path = os.path.join(base_data_dir, latest_crawled_folder, 'thumbnails')
        json_file_path = os.path.join(base_data_dir, latest_crawled_folder, 'predictions.json')
        print(f"âœ… ìµœì‹  ì´ë¯¸ì§€ í´ë” ë°œê²¬: {image_folder_path}");
        print(f"âœ… ìµœì‹  JSON íŒŒì¼ ë°œê²¬: {json_file_path}")
        if os.path.isdir(image_folder_path) and os.path.isfile(json_file_path):
            run_uploader(image_folder_path, json_file_path)
        else:
            if not os.path.isdir(image_folder_path): print(f"âŒ ì˜¤ë¥˜: ì´ë¯¸ì§€ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_folder_path}")
            if not os.path.isfile(json_file_path): print(f"âŒ ì˜¤ë¥˜: '{json_file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("âŒ í¬ë¡¤ë§ ë°ì´í„° í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € `crawler.py`ë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”.")
